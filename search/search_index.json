{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"msglc","text":"<p><code>msglc</code> is a Python library that provides a way to serialize and deserialize json objects with lazy/partial loading containers using <code>msgpack</code> as the serialization format.</p> <p>It can be used in environments that use <code>msgpack</code> to store/exchange data that is larger than a few MBs if any of the followings hold.</p> <ol> <li>After cold storage, each retrieval only accesses part of the stored data.</li> <li>Cannot afford to decode the whole file due to memory limitation, performance consideration, etc.</li> <li>Want to combine encoded data into a single blob without decoding and re-encoding the same piece of data.</li> </ol>"},{"location":"#installation","title":"Installation","text":"<p><code>msglc</code> is a pure Python library and can be installed using <code>pip</code>.</p> Bash<pre><code>pip install msglc\n</code></pre> <p>The only dependencies are <code>msgpack</code> and <code>bitarray</code>.</p>"},{"location":"#msgspec","title":"<code>msgspec</code>","text":"<p><code>msgspec</code> is an alternative library that provides better decoding performance compared to <code>msgpack</code>. It is recommended to use <code>msgspec</code>.</p> Bash<pre><code>pip install msgspec[msgspec]\n</code></pre>"},{"location":"#numpy","title":"<code>numpy</code>","text":"<p><code>numpy</code> arrays can be serialized and deserialized, to use this feature, install <code>numpy</code>.</p> Bash<pre><code>pip install msglc[numpy]\n</code></pre>"},{"location":"benchmark/","title":"Benchmark","text":"<p>The embedded structure allows fast read without loading the whole archive, which is the main advantage of this package. In the following, we benchmark the random read performance and compare with the <code>HDF5</code> format.</p>"},{"location":"benchmark/#data-generation","title":"Data Generation","text":"<p>A square matrix of size 5000 with random floating-point numbers is used. The matrix is dumped onto the disk with different configurations.</p> <ol> <li>For <code>msglc</code>, <code>small_obj_optimization_threshold</code> varies from 4KB to 4MB, <code>numpy_encoder</code> is switched off so the matrix is stored as plain json instead binary blob.</li> <li>For <code>h5py</code>, the chunk size is computed so that each block has a size similar to <code>small_obj_optimization_threshold</code>. Compression is optionally switched on.</li> </ol> <p>The following code snippets show the relevant functions.</p> Python<pre><code>def generate_msg(mat: np.ndarray, block: int):\n    configure(small_obj_optimization_threshold=2**block, numpy_encoder=False)  # 16KB\n    dump(f\"data-{block}.msg\", mat)\n\ndef generate_h5(mat: np.ndarray, block: int, **kwargs):\n    with h5py.File(h5_name(block, **kwargs), \"w\") as f:\n        if block &gt; 0:\n            chunk_size = int(sqrt(2**block / 128))\n            kwargs[\"chunks\"] = (chunk_size, chunk_size)\n        f.create_dataset(\"data\", data=mat, **kwargs)\n</code></pre> <p>The write time of <code>msglc</code> is in general constant, because the packer needs to traverse the whole json object. Depending on different configurations, <code>h5py</code> requires different amounts of time to dump the matrix.</p> <p></p> <p><code>msglc</code> shall be used for data that is written to disk for cold storage and does not require frequent changes. When compression is on, <code>h5py</code> needs to traverse the object just like <code>msglc</code>, thus requires a similar amount of time.</p>"},{"location":"benchmark/#read-test","title":"Read Test","text":"<p>We mainly test the random read. To this end, we repeatedly read random locations in the matrix and measure the time required.</p> Python<pre><code>@timeit\ndef read_msg(file: str):\n    with LazyReader(file, unpacker=MsgspecUnpacker, cached=False) as reader:\n        for _ in range(repeat):\n            reader[random.randint(0, 4999)][random.randint(0, 4999)]\n\n\n@timeit\ndef read_h5(file: str):\n    with h5py.File(file, \"r\") as f:\n        dataset = f[\"data\"]\n        for _ in range(repeat):\n            dataset[random.randint(0, 4999)][random.randint(0, 4999)]`\n</code></pre> <p>The following is the result of reading 1000 elements.</p> <p></p> <p>The following is the result of reading 10000 elements.</p> <p></p>"},{"location":"motivation/","title":"Motivation","text":"<p>MessagePack is a binary serialization format. It can be used for data exchange and storage.</p> <p>When it is used for storing large tree-like data, it has a significant shortcoming. That is, whenever one wants to read some data from cold storage, even if only a small amount of data is required, the whole binary blob needs to be de-serialized.</p> <p>This is due to the fact that <code>MessagePack</code> itself does not generate/store any structural metadata regarding the data to be serialized. As a result, there is no way to know when the desired segment is located in the binary blob. Thus, the whole thing needs to be decoded. (To be precise, it is actually linear complexity, the worse case is to decode the whole thing.)</p>"},{"location":"motivation/#structural-information","title":"Structural Information","text":"<p>Similar to the design of the zip format, it is possible to append a table of contents to the binary blob. The table of contents contains the structural information of the serialized data, serves as a lookup table, and is encoded as well.</p> <p>Whenever read operations are required, the table of contents is read first. Ideally, the table of contents is much smaller than the original binary blob. Thus, the overhead of reading the table of contents is negligible. After the table of contents is read, the desired segment can be located and decoded directly. This would significantly reduce the amount of data that needs to be decoded if only a small amount of data is required.</p>"},{"location":"motivation/#primitive-types-and-small-containers","title":"Primitive Types and Small Containers","text":"<p>For primitive types, the table of contents stores the start and end positions of the serialized data.</p> <p>For example, the table of contents of the integer <code>a=10251585</code> would look like as follows.</p> Python<pre><code>{\n  \"p\": [start, end]\n}\n</code></pre> <p>The <code>p</code> stands for position, and only a single character is used to save space.</p> <p>This applies to small containers.</p>"},{"location":"motivation/#dictionarymap","title":"Dictionary/Map","text":"<p>Apart from the <code>p</code> field that stores the start and end positions of the whole dictionary, to allow nested lookups, the table of contents also contains the tables of contents of the children under the <code>t</code> field.</p> <p>For example, the table of contents of the dictionary <code>d={\"a\": 1, \"b\": 2}</code> would look like as follows.</p> Python<pre><code>{\n  \"p\": [start, end],\n  \"t\": {\n    \"a\": {\"p\": [start, end]},\n    \"b\": {\"p\": [start, end]}\n  }\n}\n</code></pre> <p>The <code>t</code> stands for table, and only a single character is used to save space. The <code>t</code> field is a dictionary.</p>"},{"location":"motivation/#listarray","title":"List/Array","text":"<p>Similar to the dictionary, the table of contents of a list also contains the tables of contents of the children.</p> <p>For example, the table of contents of the list <code>l=[1, 2, 3]</code> would look like as follows.</p> Python<pre><code>{\n  \"p\": [start, end],\n  \"t\": [\n    {\"p\": [start, end]},\n    {\"p\": [start, end]},\n    {\"p\": [start, end]}\n  ]\n}\n</code></pre> <p>The <code>t</code> field is a list.</p>"},{"location":"motivation/#list-of-small-objects","title":"List of Small Objects","text":"<p>For lists of small objects, storing the table of contents of each child is not efficient. An alternative format of table of contents is used.</p> <p>For example, the table of contents of the list <code>l=[x for x in range(100000)]</code> would look like as follows.</p> Python<pre><code>{\n  \"p\": [[size, start, end], [size, start, end], ...]\n}\n</code></pre> <p>The small objects are grouped together, and the <code>size</code> field is used to indicate the number of small objects in the group. The size of the group can be adjusted to achieve optimal performance.</p>"},{"location":"motivation/#nested-structures","title":"Nested Structures","text":"<p>All above formats contain a <code>p</code> field.</p> <p>To support recursively packing serialized data, the following format is also used.</p> Python<pre><code># for packing a dict of packed data\n{\n  \"t\": {\n    \"a\": start,\n    \"b\": start,\n    ...\n  }\n}\n\n# for packing a list of packed data\n{\n  \"t\": [\n    start,\n    start,\n    ...\n  ]\n}\n</code></pre> <p>Since the packed data is already serialized, it would contain the table of contents. It is only necessary to store the start position of the packed data.</p>"},{"location":"motivation/#small-objects","title":"Small Objects","text":"<p>If the data to be serialized is mainly structure, generating the corresponding table of contents would potentially result in a table of contents that is larger than the original data.</p> <p>It is possible to define a threshold, such that for any given node in the tree, if the size of the node is smaller than the threshold, the node is considered a small object. For small objects, the <code>t</code> field is omitted.</p> <p>The following is an example.</p> Python<pre><code># data\n{\n  \"id\": [\n    {\n      \"BlYFs\": {\n        \"KNzFKfIR2\": [True, False],\n        \"DZFf0InHcO\": {\"t32qEJJPII\": 820701623, \"RuUbcdXGT\": 0.07535274189499452},\n      },\n      \"SWCWj\": {\n        \"T5Jm7j1p99\": {\"yEsYr8Ww\": \"1lgCDlDR\", \"1041dt7DYk\": \"XQUFG\"},\n        \"ZJejJRP\": {\"SCIVA7Lb\": 0.5045895502672991, \"p5I3XN3\": True},\n      },\n    },\n    {\n      \"vRpNA5\": {\n        \"0HNVOgUVHs\": {\"EsvObl4Q3\": -1008950541, \"SacDVqMG\": -764697401},\n        \"XLK694\": {\"UdRKNQBrku\": \"64jiA4nTf\", \"dTPdzp7Cd\": \"bC6R6Q\"},\n      },\n      \"3uyABlBlY\": {\"7umSPsl7\": {\"gFa9yuPyQ\": 0.24175848344688433, \"UYa6UiMDZ7\": True}, \"zuP2wLok\": \"G9k2y\"},\n    },\n  ]\n}\n\n# table of contents (full)\n{\n  \"t\": {\n    \"id\": {\n      \"t\": [\n        {\n          \"t\": {\n            \"BlYFs\": {\n              \"t\": {\n                \"KNzFKfIR2\": {\"p\": [23, 26]},\n                \"DZFf0InHcO\": {\n                  \"t\": {\"t32qEJJPII\": {\"p\": [49, 54]}, \"RuUbcdXGT\": {\"p\": [64, 73]}},\n                  \"p\": [37, 73],\n                },\n              },\n              \"p\": [12, 73],\n            },\n            \"SWCWj\": {\n              \"t\": {\n                \"T5Jm7j1p99\": {\n                  \"t\": {\"yEsYr8Ww\": {\"p\": [101, 110]}, \"1041dt7DYk\": {\"p\": [121, 127]}},\n                  \"p\": [91, 127],\n                },\n                \"ZJejJRP\": {\n                  \"t\": {\"SCIVA7Lb\": {\"p\": [145, 154]}, \"p5I3XN3\": {\"p\": [162, 163]}},\n                  \"p\": [135, 163],\n                },\n              },\n              \"p\": [79, 163],\n            },\n          },\n          \"p\": [5, 163],\n        },\n        {\n          \"t\": {\n            \"vRpNA5\": {\n              \"t\": {\n                \"0HNVOgUVHs\": {\n                  \"t\": {\"EsvObl4Q3\": {\"p\": [194, 199]}, \"SacDVqMG\": {\"p\": [208, 213]}},\n                  \"p\": [183, 213],\n                },\n                \"XLK694\": {\n                  \"t\": {\"UdRKNQBrku\": {\"p\": [232, 242]}, \"dTPdzp7Cd\": {\"p\": [252, 259]}},\n                  \"p\": [220, 259],\n                },\n              },\n              \"p\": [171, 259],\n            },\n            \"3uyABlBlY\": {\n              \"t\": {\n                \"7umSPsl7\": {\n                  \"t\": {\"gFa9yuPyQ\": {\"p\": [290, 299]}, \"UYa6UiMDZ7\": {\"p\": [310, 311]}},\n                  \"p\": [279, 311],\n                },\n                \"zuP2wLok\": {\"p\": [320, 326]},\n              },\n              \"p\": [269, 326],\n            },\n          },\n          \"p\": [163, 326],\n        },\n      ],\n      \"p\": [4, 326],\n    }\n  },\n  \"p\": [0, 326],\n}\n\n# table of contents (minimum block size 10 bytes)\n{\n  \"t\": {\n    \"id\": {\n      \"t\": [\n        {\n          \"t\": {\n            \"BlYFs\": {\"t\": {\"KNzFKfIR2\": {\"p\": [23, 26]}, \"DZFf0InHcO\": {\"p\": [37, 73]}}, \"p\": [12, 73]},\n            \"SWCWj\": {\"t\": {\"T5Jm7j1p99\": {\"p\": [91, 127]}, \"ZJejJRP\": {\"p\": [135, 163]}}, \"p\": [79, 163]},\n          },\n          \"p\": [5, 163],\n        },\n        {\n          \"t\": {\n            \"vRpNA5\": {\n              \"t\": {\"0HNVOgUVHs\": {\"p\": [183, 213]}, \"XLK694\": {\"p\": [220, 259]}},\n              \"p\": [171, 259],\n            },\n            \"3uyABlBlY\": {\n              \"t\": {\"7umSPsl7\": {\"p\": [279, 311]}, \"zuP2wLok\": {\"p\": [320, 326]}},\n              \"p\": [269, 326],\n            },\n          },\n          \"p\": [163, 326],\n        },\n      ],\n      \"p\": [4, 326],\n    }\n  },\n  \"p\": [0, 326],\n}\n\n# table of contents (minimum block size 100 bytes)\n{\n  \"t\": {\n    \"id\": {\n      \"t\": [\n        {\"t\": {\"BlYFs\": {\"p\": [12, 73]}, \"SWCWj\": {\"p\": [79, 163]}}, \"p\": [5, 163]},\n        {\"t\": {\"vRpNA5\": {\"p\": [171, 259]}, \"3uyABlBlY\": {\"p\": [269, 326]}}, \"p\": [163, 326]},\n      ],\n      \"p\": [4, 326],\n    }\n  },\n  \"p\": [0, 326],\n}\n\n# table of contents (minimum block size 1000 bytes)\n{\"p\": [0, 326]}\n</code></pre> <p>By controlling the block size, one can determine which format shall be used for a specific node. A small block size would result in a more detailed table of contents, its size would be relatively larger. But each actual read operation would be smaller. A large block size would result in a more compact table of contents, its size would be relatively smaller. But each actual read operation would be larger.</p> <p>It is guaranteed that each read operation would fetch at least one block of data. Depending on the actual operating system and the underlying storage hardware, the optimal block size may vary. One shall experiment with different block sizes to find the optimal one.</p>"},{"location":"motivation/#internal-layout","title":"Internal Layout","text":"<p>For a single serialized file, the following layout is used.</p> Text Only<pre><code>#####################################################################\n# magic bytes # 20 bytes # encoded data # encoded table of contents #\n#####################################################################\n</code></pre> <p>It contains four parts.</p> <ol> <li>The magic bytes are used to identify the format of the file.</li> <li>The 20 bytes are used to store the start position and the length of the encoded table of contents.</li> <li>The encoded data.</li> <li>The encoded table of contents is the table of contents encoded in msgpack.</li> </ol> <p>The table of contents is placed at the end of the file to allow direct writing of the encoded data to the file. This makes the memory footprint small.</p> <p>The encoded data is self-contained. It can be read and decoded without the table of contents.</p> <p>The same layout can be recursively used.</p>"},{"location":"tutorial/","title":"Examples","text":""},{"location":"tutorial/#serialization","title":"Serialization","text":""},{"location":"tutorial/#dumping-one-object-to-a-file","title":"Dumping one object to a file","text":"<p>Use <code>dump</code> to serialize a json object to a file.</p> Python<pre><code>from msglc import dump\n\ndata = {\"a\": [1, 2, 3], \"b\": {\"c\": 4, \"d\": 5, \"e\": [0x221548313] * 10}}\ndump(\"data.msg\", data)\n</code></pre>"},{"location":"tutorial/#combining-several-files","title":"Combining several files","text":"<p>Use <code>combine</code> to combine several serialized files together. The combined files can be further combined.</p>"},{"location":"tutorial/#combine-as-dict","title":"Combine as <code>dict</code>","text":"Python<pre><code>from msglc import dump, combine, FileInfo\nfrom msglc.reader import LazyReader\n\ndump(\"dict.msg\", {str(v): v for v in range(1000)})\ndump(\"list.msg\", [float(v) for v in range(1000)])\n\ncombine(\"combined.msg\", [FileInfo(\"dict.msg\", \"dict\"), FileInfo(\"list.msg\", \"list\")])\n# support recursively combining files\n# ...\n\n# the combined file uses a dict layout\n# { 'dict' : {'1':1,'2':2,...}, 'list' : [1.0,2.0,3.0,...] }\n# so one can read it as follows, details in coming section\nwith LazyReader(\"combined.msg\") as reader:\n    assert reader['dict/101'] == 101  # also reader['dict'][101]\n    assert reader['list/101'] == 101.0  # also reader['list'][101]\n</code></pre>"},{"location":"tutorial/#combine-as-list","title":"Combine as <code>list</code>","text":"Python<pre><code>from msglc import dump, combine, FileInfo\nfrom msglc.reader import LazyReader\n\ndump(\"dict.msg\", {str(v): v for v in range(1000)})\ndump(\"list.msg\", [float(v) for v in range(1000)])\n\ncombine(\"combined.msg\", [FileInfo(\"dict.msg\"), FileInfo(\"list.msg\")])\n# support recursively combining files\n# ...\n\n# the combined file uses a list layout\n# [ {'1':1,'2':2,...}, [1.0,2.0,3.0,...] ]\n# so one can read it as follows, details in coming section\nwith LazyReader(\"combined.msg\") as reader:\n    assert reader['0/101'] == 101  # also reader[0][101]\n    assert reader['1/101'] == 101.0  # also reader[1][101]\n</code></pre>"},{"location":"tutorial/#deserialization","title":"Deserialization","text":"<p>Use <code>LazyReader</code> to read a file.</p> Python<pre><code>from msglc.reader import LazyReader, to_obj\n\nwith LazyReader(\"data.msg\") as reader:\n    data = reader.read()  # return a LazyDict, LazyList, dict, list or primitive value\n    data = reader[\"b/c\"]  # subscriptable if the actual data is subscriptable\n    # data = reader[2:]  # also support slicing if its underlying data is list compatible\n    data = reader.read(\"b/c\")  # or provide a path to visit a particular node\n    print(data)  # 4\n    b_dict = reader.read(\"b\")\n    print(b_dict.__class__)  # &lt;class 'msglc.reader.LazyDict'&gt;\n    for k, v in b_dict.items():  # dict compatible\n        if k != \"e\":\n            print(k, v)  # c 4, d 5\n    b_json = to_obj(b_dict)  # ensure plain dict\n</code></pre>"},{"location":"api/LazyCombiner/","title":"LazyCombiner","text":"Source code in <code>src/msglc/writer.py</code> Python<pre><code>class LazyCombiner:\n    def __init__(\n        self, buffer_or_path: str | BufferWriter, *, mode: Literal[\"a\", \"w\"] = \"w\"\n    ):\n        \"\"\"\n        The mode resembles typical mode designations and implies the same meaning.\n        If the mode is 'w', the file is overwritten.\n        If the mode is 'a', the file is appended.\n\n        :param buffer_or_path: target buffer or file path\n        :param mode: mode of operation, 'w' for write and 'a' for append\n        \"\"\"\n        self._buffer_or_path: str | BufferWriter = buffer_or_path\n        self._mode: str = mode\n\n        self._buffer: BufferWriter = None  # type: ignore\n\n        self._toc: dict | list = None  # type: ignore\n        self._header_start: int = 0\n        self._file_start: int = 0\n\n    def __enter__(self):\n        if isinstance(self._buffer_or_path, str):\n            mode: str = (\n                \"wb\"\n                if not os.path.exists(self._buffer_or_path) or self._mode == \"w\"\n                else \"r+b\"\n            )\n            self._buffer = open(\n                self._buffer_or_path, mode, buffering=config.write_buffer_size\n            )\n        elif isinstance(self._buffer_or_path, (BytesIO, BufferedReader)):\n            self._buffer = self._buffer_or_path\n            if self._mode == \"a\":\n                self._buffer.seek(0)\n        else:\n            raise ValueError(\"Expecting a buffer or path.\")\n\n        if self._mode == \"w\":\n            self._buffer.write(LazyWriter.magic)\n            self._header_start = self._buffer.tell()\n            self._buffer.write(b\"\\0\" * 20)\n            self._file_start = self._buffer.tell()\n        else:\n            sep_a, sep_b, sep_c = (\n                LazyWriter.magic_len(),\n                LazyWriter.magic_len() + 10,\n                LazyWriter.magic_len() + 20,\n            )\n\n            ini_position: int = self._buffer.tell()\n            header: bytes = self._buffer.read(sep_c)\n\n            def _raise_invalid(msg: str):\n                self._buffer.seek(ini_position)\n                raise ValueError(msg)\n\n            if header[:sep_a] != LazyWriter.magic:\n                _raise_invalid(\n                    \"Invalid file format, cannot append to the current file.\"\n                )\n\n            toc_start: int = unpackb(header[sep_a:sep_b].lstrip(b\"\\0\"))\n            toc_size: int = unpackb(header[sep_b:sep_c].lstrip(b\"\\0\"))\n\n            self._buffer.seek(ini_position + sep_c + toc_start)\n            self._toc = unpackb(self._buffer.read(toc_size)).get(\"t\", None)\n\n            if isinstance(self._toc, list):\n                if any(not isinstance(i, int) for i in self._toc):\n                    _raise_invalid(\"The given file is not a valid combined file.\")\n            elif isinstance(self._toc, dict):\n                if any(not isinstance(i, int) for i in self._toc.values()):\n                    _raise_invalid(\"The given file is not a valid combined file.\")\n            else:\n                _raise_invalid(\"The given file is not a valid combined file.\")\n\n            self._header_start = ini_position + sep_a\n            self._file_start = ini_position + sep_c\n            self._buffer.seek(ini_position + sep_c + toc_start)\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        toc_start: int = self._buffer.tell() - self._file_start\n        packed_toc: bytes = packb({\"t\": self._toc})\n\n        self._buffer.write(packed_toc)\n        self._buffer.seek(self._header_start)\n        self._buffer.write(packb(toc_start).rjust(10, b\"\\0\"))\n        self._buffer.write(packb(len(packed_toc)).rjust(10, b\"\\0\"))\n\n        if isinstance(self._buffer_or_path, str):\n            self._buffer.close()\n\n    def write(self, obj: Generator, name: str | None = None) -&gt; None:\n        \"\"\"\n        Write a number of objects to the file.\n\n        :param obj: a generator of objects to be written to the file\n        :param name: a name to be assigned to the object, only required when combining in dict mode\n        \"\"\"\n        if self._toc is None:\n            self._toc = [] if name is None else {}\n\n        if name is None:\n            if not isinstance(self._toc, list):\n                raise ValueError(\"Need a name when combining in dict mode.\")\n        else:\n            if not isinstance(self._toc, dict):\n                raise ValueError(\"Cannot assign a name when combining in list mode.\")\n            if name in self._toc:\n                raise ValueError(f\"File {name} already exists.\")\n\n        start: int = self._buffer.tell() - self._file_start\n        for chunk in obj:\n            self._buffer.write(chunk)\n\n        if name is None:\n            self._toc.append(start)\n        else:\n            self._toc[name] = start\n</code></pre>"},{"location":"api/LazyCombiner/#msglc.writer.LazyCombiner.__init__","title":"<code>__init__(buffer_or_path, *, mode='w')</code>","text":"<p>The mode resembles typical mode designations and implies the same meaning. If the mode is 'w', the file is overwritten. If the mode is 'a', the file is appended.</p> <p>Parameters:</p> Name Type Description Default <code>buffer_or_path</code> <code>str | BufferWriter</code> <p>target buffer or file path</p> required <code>mode</code> <code>Literal['a', 'w']</code> <p>mode of operation, 'w' for write and 'a' for append</p> <code>'w'</code> Source code in <code>src/msglc/writer.py</code> Python<pre><code>def __init__(\n    self, buffer_or_path: str | BufferWriter, *, mode: Literal[\"a\", \"w\"] = \"w\"\n):\n    \"\"\"\n    The mode resembles typical mode designations and implies the same meaning.\n    If the mode is 'w', the file is overwritten.\n    If the mode is 'a', the file is appended.\n\n    :param buffer_or_path: target buffer or file path\n    :param mode: mode of operation, 'w' for write and 'a' for append\n    \"\"\"\n    self._buffer_or_path: str | BufferWriter = buffer_or_path\n    self._mode: str = mode\n\n    self._buffer: BufferWriter = None  # type: ignore\n\n    self._toc: dict | list = None  # type: ignore\n    self._header_start: int = 0\n    self._file_start: int = 0\n</code></pre>"},{"location":"api/LazyCombiner/#msglc.writer.LazyCombiner.write","title":"<code>write(obj, name=None)</code>","text":"<p>Write a number of objects to the file.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Generator</code> <p>a generator of objects to be written to the file</p> required <code>name</code> <code>str | None</code> <p>a name to be assigned to the object, only required when combining in dict mode</p> <code>None</code> Source code in <code>src/msglc/writer.py</code> Python<pre><code>def write(self, obj: Generator, name: str | None = None) -&gt; None:\n    \"\"\"\n    Write a number of objects to the file.\n\n    :param obj: a generator of objects to be written to the file\n    :param name: a name to be assigned to the object, only required when combining in dict mode\n    \"\"\"\n    if self._toc is None:\n        self._toc = [] if name is None else {}\n\n    if name is None:\n        if not isinstance(self._toc, list):\n            raise ValueError(\"Need a name when combining in dict mode.\")\n    else:\n        if not isinstance(self._toc, dict):\n            raise ValueError(\"Cannot assign a name when combining in list mode.\")\n        if name in self._toc:\n            raise ValueError(f\"File {name} already exists.\")\n\n    start: int = self._buffer.tell() - self._file_start\n    for chunk in obj:\n        self._buffer.write(chunk)\n\n    if name is None:\n        self._toc.append(start)\n    else:\n        self._toc[name] = start\n</code></pre>"},{"location":"api/LazyReader/","title":"LazyReader","text":"<p>               Bases: <code>LazyItem</code></p> Source code in <code>src/msglc/reader.py</code> Python<pre><code>class LazyReader(LazyItem):\n    def __init__(\n        self,\n        buffer_or_path: str | BufferReader,\n        *,\n        counter: LazyStats | None = None,\n        cached: bool = True,\n        unpacker: Unpacker | None = None,\n    ):\n        \"\"\"\n        It is possible to use a customized unpacker.\n        Please inherit the `Unpacker` class from the `unpacker.py`.\n        There are already several unpackers available using different libraries.\n\n        ```py\n        class CustomUnpacker(Unpacker):\n            def decode(self, data: bytes):\n                # provide the decoding logic\n                ...\n\n        with LazyReader(\"file.msg\", unpacker=CustomUnpacker()) as reader:\n            # read the data\n            ...\n        ```\n\n        :param buffer_or_path: the buffer or path to the file\n        :param counter: the counter object for tracking the number of bytes read\n        :param cached: whether to cache the data\n        :param unpacker: the unpacker object for reading the data\n        \"\"\"\n        self._buffer_or_path: str | BufferReader = buffer_or_path\n\n        buffer: BufferReader\n        if isinstance(self._buffer_or_path, str):\n            buffer = open(self._buffer_or_path, \"rb\", buffering=config.read_buffer_size)  # noqa: SIM115\n        elif isinstance(self._buffer_or_path, (BytesIO, BufferedReader, MockIO)):\n            buffer = self._buffer_or_path\n        else:\n            raise ValueError(\"Expecting a buffer or path.\")\n\n        sep_a, sep_b, sep_c = (\n            LazyWriter.magic_len(),\n            LazyWriter.magic_len() + 10,\n            LazyWriter.magic_len() + 20,\n        )\n\n        # keep the buffer unchanged in case of failure\n        original_pos: int = buffer.tell()\n        header: bytes = buffer.read(sep_c)\n        buffer.seek(original_pos)\n\n        if header[:sep_a] != LazyWriter.magic:\n            raise ValueError(\"Invalid file format.\")\n\n        super().__init__(\n            buffer,\n            original_pos + sep_c,\n            counter=counter,\n            cached=cached,\n            unpacker=unpacker,\n        )\n\n        toc_start: int = self._unpack(header[sep_a:sep_b].lstrip(b\"\\0\"))\n        toc_size: int = self._unpack(header[sep_b:sep_c].lstrip(b\"\\0\"))\n\n        self._obj = self._child(self._read(toc_start, toc_start + toc_size))\n\n    def __repr__(self):\n        file_path: str = \"\"\n        if isinstance(self._buffer_or_path, str):\n            file_path = \" (\" + self._buffer_or_path + \")\"\n\n        return (\n            f\"LazyReader{file_path}\"\n            if config.simple_repr or not self._cached\n            else self.to_obj().__repr__()\n        )\n\n    def __enter__(self):\n        increment_gc_counter()\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        decrement_gc_counter()\n\n        if isinstance(self._buffer_or_path, str):\n            self._buffer.close()\n\n    def __getitem__(self, item):\n        return self.read(item)\n\n    def __len__(self):\n        return len(self._obj)\n\n    def __contains__(self, item):\n        return item in self._obj\n\n    def get(self, key, default=None):\n        \"\"\"\n        Mimics the `get` method for dictionaries.\n        \"\"\"\n        return self._obj.get(key, default)\n\n    def keys(self):\n        \"\"\"\n        Mimics the `keys` method for dictionaries.\n        \"\"\"\n        return self._obj.keys()\n\n    def values(self):\n        \"\"\"\n        Mimics the `values` method for dictionaries.\n        \"\"\"\n        return self._obj.values()\n\n    def items(self):\n        \"\"\"\n        Mimics the `items` method for dictionaries.\n        \"\"\"\n        return self._obj.items()\n\n    def read(self, path: str | list | slice | None = None):\n        \"\"\"\n        Reads the data from the given path.\n\n        This method navigates through the data structure based on the provided path.\n        The path can be a string or a list. If it's a string, it's split into a list\n        using '/' as the separator. Each element of the list is used to navigate\n        through the data structure.\n\n        If the path is None, it returns the root object.\n\n        :param path: the path to the data to read\n        :return: The data at the given path.\n        \"\"\"\n\n        path_stack: list\n        if path is None:\n            path_stack = []\n        elif isinstance(path, str):\n            path_stack = path.split(\"/\")\n        elif isinstance(path, list):\n            path_stack = path\n        else:\n            path_stack = [path]\n\n        target = self._obj\n        for key in (v for v in path_stack if v != \"\"):\n            target = target[\n                to_index(key, len(target))\n                if isinstance(key, str) and isinstance(target, (list, LazyList))\n                else key\n            ]\n        return target\n\n    def visit(self, path: str = \"\"):\n        \"\"\"\n        Reads the data from the given path.\n\n        This method navigates through the data structure based on the provided path.\n        The path can be a string of paths separated by '/'.\n\n        If the path is None, it returns the root object.\n\n        :param path: the path to the data to read\n        :return: The data at the given path.\n        \"\"\"\n        target = self._obj\n        for key in (v for v in path.split(\"/\") if v != \"\"):\n            target = target[\n                to_index(key, len(target))\n                if isinstance(target, (list, LazyList))\n                else key\n            ]\n        return target\n\n    async def async_read(self, path: str | list | slice | None = None):\n        \"\"\"\n        Reads the data from the given path.\n\n        This method navigates through the data structure based on the provided path.\n        The path can be a string or a list. If it's a string, it's split into a list\n        using '/' as the separator. Each element of the list is used to navigate\n        through the data structure.\n\n        If the path is None, it returns the root object.\n\n        :param path: the path to the data to read\n        :return: The data at the given path.\n        \"\"\"\n\n        path_stack: list\n        if path is None:\n            path_stack = []\n        elif isinstance(path, str):\n            path_stack = path.split(\"/\")\n        elif isinstance(path, list):\n            path_stack = path\n        else:\n            path_stack = [path]\n\n        target = self._obj\n        for key in (v for v in path_stack if v != \"\"):\n            target = await async_get(\n                target,\n                to_index(key, len(target))\n                if isinstance(key, str) and isinstance(target, (list, LazyList))\n                else key,\n            )\n        return target\n\n    async def async_visit(self, path: str = \"\"):\n        \"\"\"\n        Reads the data from the given path.\n\n        This method navigates through the data structure based on the provided path.\n        The path can be a string of paths separated by '/'.\n\n        If the path is None, it returns the root object.\n\n        :param path: the path to the data to read\n        :return: The data at the given path.\n        \"\"\"\n        target = self._obj\n        for key in (v for v in path.split(\"/\") if v != \"\"):\n            target = await async_get(\n                target,\n                to_index(key, len(target))\n                if isinstance(target, (list, LazyList))\n                else key,\n            )\n        return target\n\n    def to_obj(self):\n        \"\"\"\n        Converts the data structure to a JSON serializable object.\n        This method will read the entire data structure into memory.\n        Data returned by this method can leave the `LazyReader` context.\n        \"\"\"\n        return to_obj(self._obj)\n</code></pre>"},{"location":"api/LazyReader/#msglc.reader.LazyReader.__init__","title":"<code>__init__(buffer_or_path, *, counter=None, cached=True, unpacker=None)</code>","text":"<p>It is possible to use a customized unpacker. Please inherit the <code>Unpacker</code> class from the <code>unpacker.py</code>. There are already several unpackers available using different libraries.</p> Python<pre><code>class CustomUnpacker(Unpacker):\n    def decode(self, data: bytes):\n        # provide the decoding logic\n        ...\n\nwith LazyReader(\"file.msg\", unpacker=CustomUnpacker()) as reader:\n    # read the data\n    ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>buffer_or_path</code> <code>str | BufferReader</code> <p>the buffer or path to the file</p> required <code>counter</code> <code>LazyStats | None</code> <p>the counter object for tracking the number of bytes read</p> <code>None</code> <code>cached</code> <code>bool</code> <p>whether to cache the data</p> <code>True</code> <code>unpacker</code> <code>Unpacker | None</code> <p>the unpacker object for reading the data</p> <code>None</code> Source code in <code>src/msglc/reader.py</code> Python<pre><code>def __init__(\n    self,\n    buffer_or_path: str | BufferReader,\n    *,\n    counter: LazyStats | None = None,\n    cached: bool = True,\n    unpacker: Unpacker | None = None,\n):\n    \"\"\"\n    It is possible to use a customized unpacker.\n    Please inherit the `Unpacker` class from the `unpacker.py`.\n    There are already several unpackers available using different libraries.\n\n    ```py\n    class CustomUnpacker(Unpacker):\n        def decode(self, data: bytes):\n            # provide the decoding logic\n            ...\n\n    with LazyReader(\"file.msg\", unpacker=CustomUnpacker()) as reader:\n        # read the data\n        ...\n    ```\n\n    :param buffer_or_path: the buffer or path to the file\n    :param counter: the counter object for tracking the number of bytes read\n    :param cached: whether to cache the data\n    :param unpacker: the unpacker object for reading the data\n    \"\"\"\n    self._buffer_or_path: str | BufferReader = buffer_or_path\n\n    buffer: BufferReader\n    if isinstance(self._buffer_or_path, str):\n        buffer = open(self._buffer_or_path, \"rb\", buffering=config.read_buffer_size)  # noqa: SIM115\n    elif isinstance(self._buffer_or_path, (BytesIO, BufferedReader, MockIO)):\n        buffer = self._buffer_or_path\n    else:\n        raise ValueError(\"Expecting a buffer or path.\")\n\n    sep_a, sep_b, sep_c = (\n        LazyWriter.magic_len(),\n        LazyWriter.magic_len() + 10,\n        LazyWriter.magic_len() + 20,\n    )\n\n    # keep the buffer unchanged in case of failure\n    original_pos: int = buffer.tell()\n    header: bytes = buffer.read(sep_c)\n    buffer.seek(original_pos)\n\n    if header[:sep_a] != LazyWriter.magic:\n        raise ValueError(\"Invalid file format.\")\n\n    super().__init__(\n        buffer,\n        original_pos + sep_c,\n        counter=counter,\n        cached=cached,\n        unpacker=unpacker,\n    )\n\n    toc_start: int = self._unpack(header[sep_a:sep_b].lstrip(b\"\\0\"))\n    toc_size: int = self._unpack(header[sep_b:sep_c].lstrip(b\"\\0\"))\n\n    self._obj = self._child(self._read(toc_start, toc_start + toc_size))\n</code></pre>"},{"location":"api/LazyReader/#msglc.reader.LazyReader.async_read","title":"<code>async_read(path=None)</code>  <code>async</code>","text":"<p>Reads the data from the given path.</p> <p>This method navigates through the data structure based on the provided path. The path can be a string or a list. If it's a string, it's split into a list using '/' as the separator. Each element of the list is used to navigate through the data structure.</p> <p>If the path is None, it returns the root object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | list | slice | None</code> <p>the path to the data to read</p> <code>None</code> <p>Returns:</p> Type Description <p>The data at the given path.</p> Source code in <code>src/msglc/reader.py</code> Python<pre><code>async def async_read(self, path: str | list | slice | None = None):\n    \"\"\"\n    Reads the data from the given path.\n\n    This method navigates through the data structure based on the provided path.\n    The path can be a string or a list. If it's a string, it's split into a list\n    using '/' as the separator. Each element of the list is used to navigate\n    through the data structure.\n\n    If the path is None, it returns the root object.\n\n    :param path: the path to the data to read\n    :return: The data at the given path.\n    \"\"\"\n\n    path_stack: list\n    if path is None:\n        path_stack = []\n    elif isinstance(path, str):\n        path_stack = path.split(\"/\")\n    elif isinstance(path, list):\n        path_stack = path\n    else:\n        path_stack = [path]\n\n    target = self._obj\n    for key in (v for v in path_stack if v != \"\"):\n        target = await async_get(\n            target,\n            to_index(key, len(target))\n            if isinstance(key, str) and isinstance(target, (list, LazyList))\n            else key,\n        )\n    return target\n</code></pre>"},{"location":"api/LazyReader/#msglc.reader.LazyReader.async_visit","title":"<code>async_visit(path='')</code>  <code>async</code>","text":"<p>Reads the data from the given path.</p> <p>This method navigates through the data structure based on the provided path. The path can be a string of paths separated by '/'.</p> <p>If the path is None, it returns the root object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the path to the data to read</p> <code>''</code> <p>Returns:</p> Type Description <p>The data at the given path.</p> Source code in <code>src/msglc/reader.py</code> Python<pre><code>async def async_visit(self, path: str = \"\"):\n    \"\"\"\n    Reads the data from the given path.\n\n    This method navigates through the data structure based on the provided path.\n    The path can be a string of paths separated by '/'.\n\n    If the path is None, it returns the root object.\n\n    :param path: the path to the data to read\n    :return: The data at the given path.\n    \"\"\"\n    target = self._obj\n    for key in (v for v in path.split(\"/\") if v != \"\"):\n        target = await async_get(\n            target,\n            to_index(key, len(target))\n            if isinstance(target, (list, LazyList))\n            else key,\n        )\n    return target\n</code></pre>"},{"location":"api/LazyReader/#msglc.reader.LazyReader.get","title":"<code>get(key, default=None)</code>","text":"<p>Mimics the <code>get</code> method for dictionaries.</p> Source code in <code>src/msglc/reader.py</code> Python<pre><code>def get(self, key, default=None):\n    \"\"\"\n    Mimics the `get` method for dictionaries.\n    \"\"\"\n    return self._obj.get(key, default)\n</code></pre>"},{"location":"api/LazyReader/#msglc.reader.LazyReader.items","title":"<code>items()</code>","text":"<p>Mimics the <code>items</code> method for dictionaries.</p> Source code in <code>src/msglc/reader.py</code> Python<pre><code>def items(self):\n    \"\"\"\n    Mimics the `items` method for dictionaries.\n    \"\"\"\n    return self._obj.items()\n</code></pre>"},{"location":"api/LazyReader/#msglc.reader.LazyReader.keys","title":"<code>keys()</code>","text":"<p>Mimics the <code>keys</code> method for dictionaries.</p> Source code in <code>src/msglc/reader.py</code> Python<pre><code>def keys(self):\n    \"\"\"\n    Mimics the `keys` method for dictionaries.\n    \"\"\"\n    return self._obj.keys()\n</code></pre>"},{"location":"api/LazyReader/#msglc.reader.LazyReader.read","title":"<code>read(path=None)</code>","text":"<p>Reads the data from the given path.</p> <p>This method navigates through the data structure based on the provided path. The path can be a string or a list. If it's a string, it's split into a list using '/' as the separator. Each element of the list is used to navigate through the data structure.</p> <p>If the path is None, it returns the root object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | list | slice | None</code> <p>the path to the data to read</p> <code>None</code> <p>Returns:</p> Type Description <p>The data at the given path.</p> Source code in <code>src/msglc/reader.py</code> Python<pre><code>def read(self, path: str | list | slice | None = None):\n    \"\"\"\n    Reads the data from the given path.\n\n    This method navigates through the data structure based on the provided path.\n    The path can be a string or a list. If it's a string, it's split into a list\n    using '/' as the separator. Each element of the list is used to navigate\n    through the data structure.\n\n    If the path is None, it returns the root object.\n\n    :param path: the path to the data to read\n    :return: The data at the given path.\n    \"\"\"\n\n    path_stack: list\n    if path is None:\n        path_stack = []\n    elif isinstance(path, str):\n        path_stack = path.split(\"/\")\n    elif isinstance(path, list):\n        path_stack = path\n    else:\n        path_stack = [path]\n\n    target = self._obj\n    for key in (v for v in path_stack if v != \"\"):\n        target = target[\n            to_index(key, len(target))\n            if isinstance(key, str) and isinstance(target, (list, LazyList))\n            else key\n        ]\n    return target\n</code></pre>"},{"location":"api/LazyReader/#msglc.reader.LazyReader.to_obj","title":"<code>to_obj()</code>","text":"<p>Converts the data structure to a JSON serializable object. This method will read the entire data structure into memory. Data returned by this method can leave the <code>LazyReader</code> context.</p> Source code in <code>src/msglc/reader.py</code> Python<pre><code>def to_obj(self):\n    \"\"\"\n    Converts the data structure to a JSON serializable object.\n    This method will read the entire data structure into memory.\n    Data returned by this method can leave the `LazyReader` context.\n    \"\"\"\n    return to_obj(self._obj)\n</code></pre>"},{"location":"api/LazyReader/#msglc.reader.LazyReader.values","title":"<code>values()</code>","text":"<p>Mimics the <code>values</code> method for dictionaries.</p> Source code in <code>src/msglc/reader.py</code> Python<pre><code>def values(self):\n    \"\"\"\n    Mimics the `values` method for dictionaries.\n    \"\"\"\n    return self._obj.values()\n</code></pre>"},{"location":"api/LazyReader/#msglc.reader.LazyReader.visit","title":"<code>visit(path='')</code>","text":"<p>Reads the data from the given path.</p> <p>This method navigates through the data structure based on the provided path. The path can be a string of paths separated by '/'.</p> <p>If the path is None, it returns the root object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the path to the data to read</p> <code>''</code> <p>Returns:</p> Type Description <p>The data at the given path.</p> Source code in <code>src/msglc/reader.py</code> Python<pre><code>def visit(self, path: str = \"\"):\n    \"\"\"\n    Reads the data from the given path.\n\n    This method navigates through the data structure based on the provided path.\n    The path can be a string of paths separated by '/'.\n\n    If the path is None, it returns the root object.\n\n    :param path: the path to the data to read\n    :return: The data at the given path.\n    \"\"\"\n    target = self._obj\n    for key in (v for v in path.split(\"/\") if v != \"\"):\n        target = target[\n            to_index(key, len(target))\n            if isinstance(target, (list, LazyList))\n            else key\n        ]\n    return target\n</code></pre>"},{"location":"api/LazyWriter/","title":"LazyWriter","text":"Source code in <code>src/msglc/writer.py</code> Python<pre><code>class LazyWriter:\n    magic: bytes = b\"msglc-2024\".rjust(max_magic_len, b\"\\0\")\n\n    @classmethod\n    def magic_len(cls) -&gt; int:\n        return len(cls.magic)\n\n    @classmethod\n    def set_magic(cls, magic: bytes):\n        cls.magic = magic.rjust(max_magic_len, b\"\\0\")\n\n    def __init__(self, buffer_or_path: str | BufferWriter, packer: Packer = None):\n        \"\"\"\n        It is possible to provide a custom packer object to be used for packing the object.\n        However, this packer must be compatible with the `msgpack` packer.\n\n        :param buffer_or_path: target buffer or file path\n        :param packer: packer object to be used for packing the object\n        \"\"\"\n        self._buffer_or_path: str | BufferWriter = buffer_or_path\n        self._packer = packer if packer else Packer()\n\n        self._buffer: BufferWriter = None  # type: ignore\n        self._toc_packer: TOC = None  # type: ignore\n        self._header_start: int = 0\n        self._file_start: int = 0\n        self._no_more_writes: bool = False\n\n    def __enter__(self):\n        increment_gc_counter()\n\n        if isinstance(self._buffer_or_path, str):\n            self._buffer = open(\n                self._buffer_or_path, \"wb\", buffering=config.write_buffer_size\n            )\n        elif isinstance(self._buffer_or_path, (BytesIO, BufferedReader)):\n            self._buffer = self._buffer_or_path\n        else:\n            raise ValueError(\"Expecting a buffer or path.\")\n\n        self._buffer.write(self.magic)\n        self._header_start = self._buffer.tell()\n        self._buffer.write(b\"\\0\" * 20)\n        self._file_start = self._buffer.tell()\n\n        self._toc_packer = TOC(packer=self._packer, buffer=self._buffer)\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        decrement_gc_counter()\n\n        if isinstance(self._buffer_or_path, str):\n            self._buffer.close()\n\n    def write(self, obj) -&gt; None:\n        \"\"\"\n        This function is used to write the object to the file.\n\n        Only one write is allowed. The function raises a `ValueError` if it is called more than once.\n\n        :param obj: the object to be written to the file\n        :raise ValueError: if the function is called more than once\n        :return: None\n        \"\"\"\n        if self._no_more_writes:\n            raise ValueError(\"No more writes allowed.\")\n\n        self._no_more_writes = True\n\n        toc: dict = self._toc_packer.pack(obj)\n        toc_start: int = self._buffer.tell() - self._file_start\n        packed_toc: bytes = self._packer.pack(toc)\n\n        self._buffer.write(packed_toc)\n        self._buffer.seek(self._header_start)\n        self._buffer.write(self._packer.pack(toc_start).rjust(10, b\"\\0\"))\n        self._buffer.write(self._packer.pack(len(packed_toc)).rjust(10, b\"\\0\"))\n</code></pre>"},{"location":"api/LazyWriter/#msglc.writer.LazyWriter.__init__","title":"<code>__init__(buffer_or_path, packer=None)</code>","text":"<p>It is possible to provide a custom packer object to be used for packing the object. However, this packer must be compatible with the <code>msgpack</code> packer.</p> <p>Parameters:</p> Name Type Description Default <code>buffer_or_path</code> <code>str | BufferWriter</code> <p>target buffer or file path</p> required <code>packer</code> <code>Packer</code> <p>packer object to be used for packing the object</p> <code>None</code> Source code in <code>src/msglc/writer.py</code> Python<pre><code>def __init__(self, buffer_or_path: str | BufferWriter, packer: Packer = None):\n    \"\"\"\n    It is possible to provide a custom packer object to be used for packing the object.\n    However, this packer must be compatible with the `msgpack` packer.\n\n    :param buffer_or_path: target buffer or file path\n    :param packer: packer object to be used for packing the object\n    \"\"\"\n    self._buffer_or_path: str | BufferWriter = buffer_or_path\n    self._packer = packer if packer else Packer()\n\n    self._buffer: BufferWriter = None  # type: ignore\n    self._toc_packer: TOC = None  # type: ignore\n    self._header_start: int = 0\n    self._file_start: int = 0\n    self._no_more_writes: bool = False\n</code></pre>"},{"location":"api/LazyWriter/#msglc.writer.LazyWriter.write","title":"<code>write(obj)</code>","text":"<p>This function is used to write the object to the file.</p> <p>Only one write is allowed. The function raises a <code>ValueError</code> if it is called more than once.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>the object to be written to the file</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the function is called more than once</p> Source code in <code>src/msglc/writer.py</code> Python<pre><code>def write(self, obj) -&gt; None:\n    \"\"\"\n    This function is used to write the object to the file.\n\n    Only one write is allowed. The function raises a `ValueError` if it is called more than once.\n\n    :param obj: the object to be written to the file\n    :raise ValueError: if the function is called more than once\n    :return: None\n    \"\"\"\n    if self._no_more_writes:\n        raise ValueError(\"No more writes allowed.\")\n\n    self._no_more_writes = True\n\n    toc: dict = self._toc_packer.pack(obj)\n    toc_start: int = self._buffer.tell() - self._file_start\n    packed_toc: bytes = self._packer.pack(toc)\n\n    self._buffer.write(packed_toc)\n    self._buffer.seek(self._header_start)\n    self._buffer.write(self._packer.pack(toc_start).rjust(10, b\"\\0\"))\n    self._buffer.write(self._packer.pack(len(packed_toc)).rjust(10, b\"\\0\"))\n</code></pre>"},{"location":"api/utility/","title":"Utility Functions","text":"<p>The following are main utility functions to create archive.</p>"},{"location":"api/utility/#dump","title":"dump","text":"<p>This function is used to write the object to the file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | BytesIO</code> <p>a string representing the file path</p> required <code>obj</code> <p>the object to be written to the file</p> required <code>kwargs</code> <p>additional keyword arguments to be passed to the <code>LazyWriter</code></p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/msglc/__init__.py</code> Python<pre><code>def dump(file: str | BytesIO, obj, **kwargs):\n    \"\"\"\n    This function is used to write the object to the file.\n\n    :param file: a string representing the file path\n    :param obj: the object to be written to the file\n    :param kwargs: additional keyword arguments to be passed to the `LazyWriter`\n    :return: None\n    \"\"\"\n    with LazyWriter(file, **kwargs) as msglc_writer:\n        msglc_writer.write(obj)\n</code></pre>"},{"location":"api/utility/#combine","title":"combine","text":"<p>This function is used to combine the multiple serialized files into a single archive.</p> <p>Parameters:</p> Name Type Description Default <code>archive</code> <code>str | BytesIO</code> <p>a string representing the file path of the archive</p> required <code>files</code> <code>FileInfo | list[FileInfo]</code> <p>a list of FileInfo objects</p> required <code>mode</code> <code>Literal['a', 'w']</code> <p>a string representing the combination mode, 'w' for write and 'a' for append</p> <code>'w'</code> <code>validate</code> <code>bool</code> <p>switch on to validate the files before combining</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/msglc/__init__.py</code> Python<pre><code>def combine(\n    archive: str | BytesIO,\n    files: FileInfo | list[FileInfo],\n    *,\n    mode: Literal[\"a\", \"w\"] = \"w\",\n    validate: bool = True,\n):\n    \"\"\"\n    This function is used to combine the multiple serialized files into a single archive.\n\n    :param archive: a string representing the file path of the archive\n    :param files: a list of FileInfo objects\n    :param mode: a string representing the combination mode, 'w' for write and 'a' for append\n    :param validate: switch on to validate the files before combining\n    :return: None\n    \"\"\"\n    if isinstance(files, FileInfo):\n        files = [files]\n\n    if 0 &lt; sum(1 for file in files if file.name is not None) &lt; len(files):\n        raise ValueError(\"Files must either all have names or all not have names.\")\n\n    if len(all_names := {file.name for file in files}) != len(files) and (\n        len(all_names) != 1 or all_names.pop() is not None\n    ):\n        raise ValueError(\"Files must have unique names.\")\n\n    def _validate(_fp):\n        if isinstance(_fp, str):\n            if not os.path.exists(_fp):\n                raise ValueError(f\"File {_fp} does not exist.\")\n            with open(_fp, \"rb\") as _file:\n                if _file.read(LazyWriter.magic_len()) != LazyWriter.magic:\n                    raise ValueError(f\"Invalid file format: {_fp}.\")\n        else:\n            ini_pos = _fp.tell()\n            magic = _fp.read(LazyWriter.magic_len())\n            _fp.seek(ini_pos)\n            if magic != LazyWriter.magic:\n                raise ValueError(\"Invalid file format.\")\n\n    if validate:\n        for file in files:\n            _validate(file.path)\n\n    def _iter(path: str | BinaryIO):\n        if isinstance(path, str):\n            with open(path, \"rb\") as _file:\n                while _data := _file.read(config.copy_chunk_size):\n                    yield _data\n        else:\n            while _data := path.read(config.copy_chunk_size):\n                yield _data\n\n    with LazyCombiner(archive, mode=mode) as combiner:\n        for file in files:\n            combiner.write(_iter(file.path), file.name)\n</code></pre>"},{"location":"api/utility/#append","title":"append","text":"<p>This function is used to append the multiple serialized files to an existing single archive.</p> <p>Parameters:</p> Name Type Description Default <code>archive</code> <code>str | BytesIO</code> <p>a string representing the file path of the archive</p> required <code>files</code> <code>FileInfo | list[FileInfo]</code> <p>a list of FileInfo objects</p> required <code>validate</code> <code>bool</code> <p>switch on to validate the files before combining</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/msglc/__init__.py</code> Python<pre><code>def append(\n    archive: str | BytesIO, files: FileInfo | list[FileInfo], *, validate: bool = True\n):\n    \"\"\"\n    This function is used to append the multiple serialized files to an existing single archive.\n\n    :param archive: a string representing the file path of the archive\n    :param files: a list of FileInfo objects\n    :param validate: switch on to validate the files before combining\n    :return: None\n    \"\"\"\n    combine(archive, files, mode=\"a\", validate=validate)\n</code></pre>"},{"location":"api/utility/#configure","title":"configure","text":"<p>This function is used to configure the settings. It accepts any number of keyword arguments. The function updates the values of the configuration parameters if they are provided in the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>small_obj_optimization_threshold</code> <code>int | None</code> <p>The threshold (in bytes) for small object optimization. Objects smaller than this threshold are not indexed.</p> <code>None</code> <code>write_buffer_size</code> <code>int | None</code> <p>The size (in bytes) for the write buffer.</p> <code>None</code> <code>read_buffer_size</code> <code>int | None</code> <p>The size (in bytes) for the read buffer.</p> <code>None</code> <code>fast_loading</code> <code>bool | None</code> <p>Flag to enable or disable fast loading. If enabled, the container will be read in one go, instead of reading each child separately.</p> <code>None</code> <code>fast_loading_threshold</code> <code>int | float | None</code> <p>The threshold (0 to 1) for fast loading. With the fast loading flag turned on, fast loading will be performed if the number of already read children over the total number of children is smaller than this threshold.</p> <code>None</code> <code>trivial_size</code> <code>int | None</code> <p>The size (in bytes) considered trivial, around a dozen bytes. Objects smaller than this size are considered trivial. For a list of trivial objects, the container will be indexed in a blocked fashion.</p> <code>None</code> <code>disable_gc</code> <code>bool | None</code> <p>Flag to enable or disable garbage collection.</p> <code>None</code> <code>simple_repr</code> <code>bool | None</code> <p>Flag to enable or disable simple representation used in the repr method. If turned on, repr will not incur any disk I/O.</p> <code>None</code> <code>copy_chunk_size</code> <code>int | None</code> <p>The size (in bytes) for the copy chunk.</p> <code>None</code> <code>numpy_encoder</code> <code>bool | None</code> <p>Flag to enable or disable the <code>numpy</code> support. If enabled, the <code>numpy</code> arrays will be encoded using the <code>dumps</code> method provided by <code>numpy</code>. The arrays are stored as binary data directly. If disabled, the <code>numpy</code> arrays will be converted to lists before encoding.</p> <code>None</code> <code>numpy_fast_int_pack</code> <code>bool | None</code> <p>If enabled, the integer numpy array will be packed assigning each element has identical size (4 or 8 bytes). This improves the performance of packing by avoiding the overhead of checking the size of each element. However, depending on the backend, for example, <code>messagepack</code> C implementation packs unsigned long long or long long. But its python implementation packs integer of various lengths (1, 2, 3, 5, 9 bytes).</p> <code>None</code> <code>magic</code> <code>bytes | None</code> <p>Magic bytes (max length: 30) to set, used to identify the file format version.</p> <code>None</code> Source code in <code>src/msglc/config.py</code> Python<pre><code>def configure(\n    *,\n    small_obj_optimization_threshold: int | None = None,\n    write_buffer_size: int | None = None,\n    read_buffer_size: int | None = None,\n    fast_loading: bool | None = None,\n    fast_loading_threshold: int | float | None = None,\n    trivial_size: int | None = None,\n    disable_gc: bool | None = None,\n    simple_repr: bool | None = None,\n    copy_chunk_size: int | None = None,\n    numpy_encoder: bool | None = None,\n    numpy_fast_int_pack: bool | None = None,\n    magic: bytes | None = None,\n):\n    \"\"\"\n    This function is used to configure the settings. It accepts any number of keyword arguments.\n    The function updates the values of the configuration parameters if they are provided in the arguments.\n\n    :param small_obj_optimization_threshold:\n            The threshold (in bytes) for small object optimization.\n            Objects smaller than this threshold are not indexed.\n    :param write_buffer_size:\n            The size (in bytes) for the write buffer.\n    :param read_buffer_size:\n            The size (in bytes) for the read buffer.\n    :param fast_loading:\n            Flag to enable or disable fast loading.\n            If enabled, the container will be read in one go, instead of reading each child separately.\n    :param fast_loading_threshold:\n            The threshold (0 to 1) for fast loading.\n            With the fast loading flag turned on, fast loading will be performed if the number of\n            already read children over the total number of children is smaller than this threshold.\n    :param trivial_size:\n            The size (in bytes) considered trivial, around a dozen bytes.\n            Objects smaller than this size are considered trivial.\n            For a list of trivial objects, the container will be indexed in a blocked fashion.\n    :param disable_gc:\n            Flag to enable or disable garbage collection.\n    :param simple_repr:\n            Flag to enable or disable simple representation used in the __repr__ method.\n            If turned on, __repr__ will not incur any disk I/O.\n    :param copy_chunk_size:\n            The size (in bytes) for the copy chunk.\n    :param numpy_encoder:\n            Flag to enable or disable the `numpy` support.\n            If enabled, the `numpy` arrays will be encoded using the `dumps` method provided by `numpy`.\n            The arrays are stored as binary data directly.\n            If disabled, the `numpy` arrays will be converted to lists before encoding.\n    :param numpy_fast_int_pack:\n            If enabled, the integer numpy array will be packed assigning each element has identical size (4 or 8 bytes).\n            This improves the performance of packing by avoiding the overhead of checking the size of each element.\n            However, depending on the backend, for example, `messagepack` C implementation packs unsigned long long or long long.\n            But its python implementation packs integer of various lengths (1, 2, 3, 5, 9 bytes).\n    :param magic:\n            Magic bytes (max length: 30) to set, used to identify the file format version.\n    \"\"\"\n    if (\n        isinstance(small_obj_optimization_threshold, int)\n        and small_obj_optimization_threshold &gt; 0\n    ):\n        config.small_obj_optimization_threshold = small_obj_optimization_threshold\n        if config.trivial_size &gt; config.small_obj_optimization_threshold:\n            config.trivial_size = config.small_obj_optimization_threshold\n\n    if isinstance(write_buffer_size, int) and write_buffer_size &gt; 0:\n        config.write_buffer_size = write_buffer_size\n\n    if isinstance(read_buffer_size, int) and read_buffer_size &gt; 0:\n        config.read_buffer_size = read_buffer_size\n\n    if isinstance(fast_loading, bool):\n        config.fast_loading = fast_loading\n\n    if (\n        isinstance(fast_loading_threshold, (int, float))\n        and 0 &lt;= fast_loading_threshold &lt;= 1\n    ):\n        config.fast_loading_threshold = fast_loading_threshold\n\n    if isinstance(trivial_size, int) and trivial_size &gt; 0:\n        config.trivial_size = trivial_size\n        if config.trivial_size &gt; config.small_obj_optimization_threshold:\n            config.small_obj_optimization_threshold = config.trivial_size\n\n    if isinstance(disable_gc, bool):\n        config.disable_gc = disable_gc\n\n    if isinstance(simple_repr, bool):\n        config.simple_repr = simple_repr\n\n    if isinstance(copy_chunk_size, int) and copy_chunk_size &gt; 0:\n        config.copy_chunk_size = copy_chunk_size\n\n    if isinstance(numpy_encoder, bool):\n        config.numpy_encoder = numpy_encoder\n\n    if isinstance(numpy_fast_int_pack, bool):\n        config.numpy_fast_int_pack = numpy_fast_int_pack\n\n    if isinstance(magic, bytes) and 0 &lt; len(magic) &lt;= max_magic_len:\n        from msglc import LazyWriter\n\n        LazyWriter.set_magic(magic)\n</code></pre>"}]}